{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vid2speed\n",
    "## Ben Penchas, Marco Monteiro, and Toby Bell\n",
    "Given car dashboard video footage, we aimed to estimate the speed of a car using a deep neural network. We saw this problem as a small but important part of building an autonomous vehicle. The problem is by its nature underdetermined (since we have no absolute reference for scale/distance), so we treated it as a classification task where we bucketed speeds into 4 mph intervals.\n",
    "\n",
    "<img src=\"im0.jpeg\">\n",
    "\n",
    "Our dataset was a dashboard video of driving around the Bay Area. We took our data from the <a href=\"https://twitter.com/comma_ai/status/854488327797448704?lang=en\">comma.ai challenge.</a> We separated this video into 20400 image frames (that look like the one above), scaled each image to 224x224, and coupled every pair of frames. We then randomly shuffled these pairs--80% into our train set and 20% into our test set.\n",
    "\n",
    "See below for how we implemented our final network and trained it. Our resulting accuracy was 91% on the test set. We did some analysis to understand what the network learned, and gained several insights (see bottom).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import imageio\n",
    "import cv2\n",
    "from moviepy.editor import *\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function we wrote to turn the video given to us\n",
    "# into an object of resized frames\n",
    "def extract_frames(movie, imgdir):\n",
    "    clip = VideoFileClip(movie)\n",
    "    frames = int(clip.fps * clip.duration)\n",
    "    all_frames = np.empty((frames,224*224*3))\n",
    "    for f in range(frames):\n",
    "        # Extract each frame and resize to 224x224\n",
    "        image = clip.to_ImageClip(f / clip.fps).img\n",
    "        image = scipy.misc.imresize(image, (224, 224))\n",
    "        image = image.astype('float32')\n",
    "        \n",
    "        all_frames[f,:] = image.flatten()\n",
    "        \n",
    "        if f % 1000 == 0:\n",
    "            print(f)\n",
    "    return all_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting frames look like this one:\n",
    "<img src=\"im0.jpeg\">\n",
    "\n",
    "# Network Architecture\n",
    "In the beginning, we saw the problem of dense optical flow as very similar to our video-to-speed problem, but not exactly the same. Rather than use a pretrained Flownet model, a very deep and complicated network, we wanted to design and train our own lightweight optical flow model specific to this problem.\n",
    "\n",
    "We initially used VGG-19 as a dual stream feature extractor, mimicking the Flownet architecture and hoping to extract features like scale. We fed each image through a pre-trained VGG-19 model, up to the third block, and then concatenated the results. We then fed the resulting volume through our trainable custom CNN.\n",
    "\n",
    "However, this did not work for two main reasons:\n",
    "\n",
    "1) VGG-19 is an image classifier, and so it has been trained to extract semantic understanding. However, since subsequent frames are semantically very similar, the feature extraction with VGG-19 actually got rid of crucial spatial information. Using VGG-19 as a feature extractor led to a model that could not distinguish between images.\n",
    "\n",
    "2) Additionally, VGG-19 has rapid down sampling with several max-pool layers. That architecture caused a loss of resolution in the images. For optical flow, and this video-to-speed problem, high spatial resolution is very important.\n",
    "\n",
    "Thus we decided not to use a pretrained feature extractor and to just let the network learn everything on its own. In the following cell we define our network architecture. Here is an illustration of our architecture:\n",
    "\n",
    "<img src=\"architecture.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc. config.\n",
    "\n",
    "# Log messages.\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Batch size to use when training. Note that this is the number of image\n",
    "# *pairs* that will be fed in at once (so it need not be even).\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "# Number of vid2speed speed buckets.\n",
    "NUM_LABELS = 15\n",
    "\n",
    "# Build the vid2speed classifier network. Accepts a tensor of 3-channel (RGB)\n",
    "# uint8 image data with shape [2n, 224, 224, 3] - i.e., n pairs of images.\n",
    "# Returns the logits layer.\n",
    "def vid2speed(images):\n",
    "\n",
    "    reshaped = tf.reshape(images, [-1, 2, 224, 224, 3])\n",
    "    inputs = tf.cast(reshaped, tf.float32)\n",
    "    \n",
    "    even_img = inputs[:, 0, :, :, :]\n",
    "    odd_img = inputs[:, 1, :, :, :]\n",
    "    tf.summary.image('even', even_img, max_outputs=12)\n",
    "    tf.summary.image('odd', odd_img, max_outputs=12)\n",
    "\n",
    "    concat_vol = tf.concat([even_img, odd_img], 3)\n",
    "\n",
    "    # Three convolutional layers.\n",
    "    conv_1 = tf.layers.conv2d(concat_vol,\n",
    "                              filters=64,\n",
    "                              kernel_size=3,\n",
    "                              strides=2,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name='conv_1')\n",
    "    conv_2 = tf.layers.conv2d(conv_1,\n",
    "                              filters=64,\n",
    "                              kernel_size=3,\n",
    "                              strides=2,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name='conv_2')\n",
    "    conv_3 = tf.layers.conv2d(conv_2,\n",
    "                              filters=64,\n",
    "                              kernel_size=3,\n",
    "                              strides=2,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name='conv_3')\n",
    "\n",
    "    # Four fully-connected layers.\n",
    "    flat = tf.reshape(conv_3, [-1, 46656])\n",
    "    fc_4 = tf.layers.dense(flat, 4096, activation=tf.nn.relu, name='fc_4')\n",
    "    fc_5 = tf.layers.dense(fc_4, 4096, activation=tf.nn.relu, name='fc_5')\n",
    "    fc_6 = tf.layers.dense(fc_5, 4096, activation=tf.nn.relu, name='fc_6')\n",
    "    fc_7 = tf.layers.dense(fc_6, NUM_LABELS, name='fc_7')\n",
    "\n",
    "    return fc_7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up training and evaluation using an Estimator\n",
    "\n",
    "We decided to use an Estimator to make checkpointing easy (we wanted to be able to start on stop on Floydhub without losing training progress). We use the cross entropy loss between the true bucket (discretized by us) and our predicted bucket. We use the gradient descent optimizer with learning_rate = 0.001. We tried larger values of this hyperparameter but found the loss went up during training (overshot the minimum). We apply softmax to our last layer to get probabilities for every bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the vid2speed network given an `images` batch and `labels` vector. The\n",
    "# `images` tensor should have shape [2n, 224, 224, 3] and type uint8 - i.e., a\n",
    "# batch of n pairs of 3-channel (RGB) 224x224 images. This function is written\n",
    "# to comply with the TensorFlow Estimator specification, and should be used to\n",
    "# create an Estimator for the vid2speed network.\n",
    "def vid2speed_estimator(features, labels, mode):\n",
    "    \n",
    "    # Build the network itself.\n",
    "    images = features['images']\n",
    "    logits = vid2speed(images)\n",
    "   \n",
    "    # Given the logits layer, we can form predictions using argmax to find the\n",
    "    # most likely class, and softmax to find class probabilities.\n",
    "    predictions = {\n",
    "        'classes': tf.cast(tf.argmax(logits, axis=1), tf.int32),\n",
    "        \n",
    "        # Adding `softmax_tensor` to the graph is used for predictions. It also\n",
    "        # allows for logging during training.\n",
    "        'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
    "    }\n",
    "    \n",
    "    # If we just want to predict new samples, we don't need anything else.\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions)\n",
    "    \n",
    "    # Construct the loss function based on a one_hot vector for each label and\n",
    "    # the logits layer from the vid2speed network. The loss function is\n",
    "    # necessary for training and evaluation.\n",
    "    labels = labels / 29 * NUM_LABELS\n",
    "    labels = tf.cast(labels, tf.int32)\n",
    "    one_hot = tf.one_hot(labels, NUM_LABELS)\n",
    "    loss = tf.losses.softmax_cross_entropy(one_hot, logits)\n",
    "    tf.summary.scalar('cost', loss)\n",
    "    tf.identity(loss, name='loss')\n",
    "    tf.identity(labels, name='labels')\n",
    "    tf.identity(predictions['classes'], 'preds')\n",
    "\n",
    "    # Configure the training operation using the loss function created above.\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(loss, tf.train.get_global_step())\n",
    "        \n",
    "        correct_prediction = tf.equal(labels, predictions['classes'])\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics for EVAL mode.\n",
    "    metrics = {'accuracy': tf.metrics.accuracy(labels, predictions['classes'])}\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the training\n",
    "\n",
    "We trained for over 9,000 minibatches on Floydhub (about 15 epochs). We used Tensorboard to monitor the progress and output loss/accuracy graphs (included below). We also checkpointed the model every 60 seconds. We attempted to train locally, but doing so took on the order of seconds per minibatch (unacceptably slow). So we used Floydhub to do all the training.\n",
    "\n",
    "As expected with training on minibatches, our accuracy on each batch varies but tends upward. By the end of the 15 epochs, we were able to completely fit the train set. We then evaluated on our unseen test set. Our trained model achieved  91% accuracy on the test set. Given the varied nature of roads in the train/test sets, we feel confident the network would generalize further to unseen road environments. We did not get to try re-training with regularization (such as by using or dropout); in our next iteration we would like to try regularizing the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training cell\n",
    "\n",
    "# Data import.\n",
    "# Load training and testing data.\n",
    "# Note: data tensors should have shape [2n, 224, 224, 3] and type uint8 (i.e.,\n",
    "# n pairs of 224x224 3-channel (RGB) images), and the label vectors should have\n",
    "# shape [n], containing the speed bucket for each pair of images.\n",
    "train_data = np.load('/data/X_train.npy').reshape((-1, 2, 224, 224, 3))\n",
    "train_labels = np.load('/data/y_train.npy')\n",
    "\n",
    "\n",
    "# Create the Estimator\n",
    "config = tf.estimator.RunConfig(save_checkpoints_secs=60)\n",
    "v2s_classifier = tf.estimator.Estimator(vid2speed_estimator,\n",
    "                                        './model',\n",
    "                                        config=config)\n",
    "\n",
    "# Set up logging during training. Logs will be produced every 50 training\n",
    "# iterations. Note: alternatively, logs can be produced at a fixed time\n",
    "# rate. To enable this behavior, use the `every_n_secs` parameter instead\n",
    "# of the `every_n_iter` parameter below.\n",
    "log_targets = {\n",
    "    'loss': 'loss',\n",
    "    'preds': 'preds',\n",
    "    'labels': 'labels'\n",
    "}\n",
    "logging = tf.train.LoggingTensorHook(log_targets, every_n_iter=50)\n",
    "\n",
    "'''\n",
    "# Create a training function using the training data set above.\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': train_data},\n",
    "    y=train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Run training.\n",
    "v2s_classifier.train(input_fn, steps=20000, hooks=[logging])\n",
    "'''\n",
    "\n",
    "# Create an evaluation function using the test data set above.\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': test_data},\n",
    "    y=test_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Run training.\n",
    "v2s_classifier.evaluate(input_fn, hooks=[logging])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy as a function of # iterations\n",
    "<img src=\"accuracy.png\">\n",
    "## Loss as a function of # iterations\n",
    "<img src=\"loss.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Once we fit the training data, we wanted to see what we had learned. We will now analyze the network output by creating pixel-level saliency maps. We take the gradient of the loss with respect to each pixel and visualize the output. To do so, we need to restore the graph from a checkpoint (since we are using an Estimator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter `pair` should have size [2, 224, 224, 3]. Parameter `label` should be a\n",
    "# single label value.\n",
    "def get_gradient_map(pair, label):\n",
    "    tf.reset_default_graph()\n",
    "    pair = tf.Variable(pair, name='pair', dtype=tf.float32)\n",
    "    label = np.array([label])\n",
    "    label = label / 29 * NUM_LABELS\n",
    "    logits = vid2speed(pair)\n",
    "    label = tf.cast(label, tf.int32)\n",
    "    one_hot = tf.one_hot(label, NUM_LABELS)\n",
    "    loss = tf.losses.softmax_cross_entropy(one_hot, logits)\n",
    "    with tf.Session() as sess:\n",
    "        pair.initializer.run()\n",
    "        variables = tf.trainable_variables()\n",
    "        uninit = []\n",
    "        for vari in variables:\n",
    "            if vari.name != 'pair:0':\n",
    "                uninit.append(vari)\n",
    "        saver = tf.train.Saver(uninit)\n",
    "        saver.restore(sess, './model/model.ckpt-9014')\n",
    "        grad = tf.gradients(loss, pair)\n",
    "        return np.array(sess.run(grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data = np.load('/data/X_test.npy').reshape((-1, 2, 224, 224, 3))\n",
    "test_labels = np.load('/data/y_test.npy')\n",
    "\n",
    "# Create pixel level saliency maps\n",
    "normy_index = 17\n",
    "pair = test_data[normy_index, :, :, :, :]\n",
    "mappy = get_gradient_map(pair, test_labels[normy_index])\n",
    "mappy = mappy[0, :, :, :, :]\n",
    "mappy_norms = np.linalg.norm(mappy, ord=2, axis=3)\n",
    "mappy_norms /= np.max(mappy_norms)\n",
    "mappy_norms = (mappy_norms * 255).astype('uint8')\n",
    "mappy_norms = np.expand_dims(mappy_norms, axis=3)\n",
    "mappy_norms = np.repeat(mappy_norms, 3, axis=3)\n",
    "\n",
    "im0 = Image.fromarray(pair[0, :, :, :])\n",
    "im1 = Image.fromarray(pair[1, :, :, :])\n",
    "map0 = Image.fromarray(mappy_norms[0, :, :, :])\n",
    "map1 = Image.fromarray(mappy_norms[1, :, :, :])\n",
    "im0.save('im0.png')\n",
    "im1.save('im1.png')\n",
    "map0.save('map0.png')\n",
    "map1.save('map1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peeking inside the \"black box\"\n",
    "To better understand our results, we took the gradient of the loss with respect to each pixel of an input image. We then visualized the gradients by coloring the pixels by the magnitude of the norm of their gradients (brighter here means a larger gradient). In other words, we are left with a â€œheat mapâ€ of which pixels the network focuses on to make its decision. Note we enhanced these maps with color and overlayed them on the original image to make them more visual.\n",
    "\n",
    "<img src=\"im0.jpeg\"> <img src=\"map0.jpeg\"> <img src=\"map0_overlay.jpeg\">\n",
    "\n",
    "As you can see, the network learned to ignore the road and the sky. It learned to focus on the white lines in the road (something we hypothesized it would). Surprisingly, it also focuses on the hood of the car / the top of the car. After investigating this phenomenon more, we believe the car itself shakes more at higher speeds and thus the network has learned to factor in the shaking of the car. How neat!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "Next, we would like to try regularizing the network and testing it on different driving conditions (like different cars and roads). We are somewhat confident the model will generalize because the training set had such variable road conditions in it (neighborhoods, traffic, highways, etc). We also got a pretrained optical flow network working and would have liked to try using it as a feature extractor before our network. Here is some sample output from that network:\n",
    "\n",
    "<img src=\"movement.png\">\n",
    "<img src=\"flow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In CVPR, 2017.\n",
    "\n",
    "Fischer, Philipp, Dosovitskiy, Alexey, Ilg, Eddy, Hausser, Philip, Hazrba, Caner, Golkov, Vladimir, van der Â¨ Smagt, Patrick, Cremers, Daniel, and Brox, Thomas. Flownet: Learning optical flow with convolutional neural networks. In ICCV, 2015.\n",
    "\n",
    "K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.\n",
    "\n",
    "Websites:\n",
    "\n",
    "https://github.com/pathak22/pyflow\n",
    "\n",
    "https://comma.ai/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
